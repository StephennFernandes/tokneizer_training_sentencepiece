{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\tdocker\t     Readme.md\t       trained_tokenizers\n",
      "cfg.py\tnotebooks    requirements.txt  train_tokenizer.ipynb\n",
      "config\t__pycache__  src\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/stephen/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dataset = datasets.load_from_disk(\"/home/stephen/Desktop/MEGA_CORPUS/COMBINED_CORPUS/HINDI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 65280806\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 328045\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to  work/data/hi_sentences/sent_0.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000000/10000000 [51:37<00:00, 3227.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to  work/data/hi_sentences/sent_1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000000/10000000 [52:04<00:00, 3200.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to  work/data/hi_sentences/sent_2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000000/10000000 [52:11<00:00, 3193.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to  work/data/hi_sentences/sent_3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000000/10000000 [52:01<00:00, 3203.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to  work/data/hi_sentences/sent_4.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000000/10000000 [52:22<00:00, 3181.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to  work/data/hi_sentences/sent_5.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000000/10000000 [51:23<00:00, 3242.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to  work/data/hi_sentences/sent_6.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5280806/5280806 [27:19<00:00, 3221.18it/s] \n"
     ]
    }
   ],
   "source": [
    "from src.make_datasets import make_sentence_files\n",
    "make_sentence_files(lang_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled files:\n",
      "['work/data/hi_sentences/sent_2.txt', 'work/data/hi_sentences/sent_1.txt', 'work/data/hi_sentences/sent_0.txt', 'work/data/hi_sentences/sent_4.txt', 'work/data/hi_sentences/sent_3.txt', 'work/data/hi_sentences/sent_6.txt', 'work/data/hi_sentences/sent_5.txt']\n",
      "number of lines sampled: 18135714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18135714/18135714 [08:32<00:00, 35371.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote to  work/data/temp.txt\n"
     ]
    }
   ],
   "source": [
    "import cfg\n",
    "from src.make_datasets import sample_and_make_tempfile\n",
    "\n",
    "tempfile_path = sample_and_make_tempfile(\n",
    "                                        sentences_dir = cfg.HI_SENTENCES_DIR\n",
    "                                        , num_files = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import time\n",
    "\n",
    "# train for these sizes\n",
    "vocab_sizes = [8000, 16000, 32000, 48000]\n",
    "\n",
    "def train_jp(vocab_size):\n",
    "\n",
    "    start = time.time()\n",
    "    model_prefix = \"cc100_hi\" + \"_vocab_\" + str(vocab_size)\n",
    "    spm.SentencePieceTrainer.train(input=tempfile_path\n",
    "                                           , model_prefix=model_prefix\n",
    "                                           , vocab_size=vocab_size\n",
    "                                           , character_coverage =1\n",
    "                                           , num_threads=12\n",
    "                                           , train_extremely_large_corpus=True\n",
    "                                          ) \n",
    "    print(\"Trained {} in {} seconds\".format(model_prefix, time.time()-start))\n",
    "\n",
    "# train\n",
    "for vocab_size in vocab_sizes:\n",
    "    train_jp(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
